---
title: "SCProject"
author: "Saketh,Sree Ram,Poojitha,Naveena,Supraja"
date: "2025-04-07"
output: pdf_document
---
```{r}

df <- read.csv("~/Documents/real_estate_ga.csv")
```

PRE PROCESSING

```{r}
# Display the first 4 rows of the DataFrame df.
head(df, 4)
```


```{r}
# Display the 3 random rows of the DataFrame df.
df[sample(nrow(df), 3), ]

```

```{r}
# Display the 2 random rows of the DataFrame df. Make sure that the rows stay the same if one reruns this cell.

set.seed(42)  # You can choose any number for the seed
df[sample(nrow(df), 2), ]

```


```{r}
# How many row and columns are in the dataset?
dim(df)
```
```{r}
# Print list of column names
colnames(df)
```
```{r}
# What is Unnamed: 0 column? Can we delete it without losing essential information? Why?
#The Unnamed: 0 column is just an automatically generated index from saving the DataFrame to a CSV file. It can be safely deleted without losing essential information since Pandas already manages row indexes internally.
```

```{r}
# Delete column Unnamed: 0
df$`Unnamed: 0` <- NULL
```

```{r}
# Display a summary
summary(df)
```

```{r}
# Print number of unique values in each column of the DataFrame

unique_values <- sapply(df, function(x) length(unique(x)))
print(unique_values)


```
```{r}
# Some columns contain only a single unique value. Which ones? Print their names. Then, delete all such columns and explain why removing them makes sense.

single <- sapply(df, function(col) length(unique(col)) == 1)
single_columns <- names(single[single])
cat("Columns with single unique value:", single_columns, "\n")
df <- df[, !names(df) %in% single_columns]
cat("Columns after deletion:", colnames(df), "\n")


#They generally don't provide any useful information because they donâ€™t help in distinguishing between different data entries. Keeping them can clutter the dataset and slow down data processing.
```
```{r}
# Run the code cell below. Are the results the same or different? If they differ, explain why.

all_equal <- all(df$livingArea == df$livingAreaValue)
cat("All values are equal:", all_equal, "\n")
exact_equal <- identical(df$livingArea, df$livingAreaValue)
cat("Columns are exactly equal:", exact_equal, "\n")


#It returns True but the values might differ accordinly. For suppose, the first check looks only at the values. Whereas, the second line of the code looks at values, data type, and even the missing values' positions. The order of the data might be different.
```
```{r}
#Display a summary of the numerical features again. Identify any columns with identical statistics. If such columns exist, verify whether they are true duplicates and keep only the first occurrence, removing all others.

#14)
```


```{r}
# Identify all columns that contain exactly two unique values in the DataFrame and retrieve the unique values present in each of them.

df_2unique <- df[sapply(df, function(x) length(unique(x)) == 2)]
unique_values <- sapply(df_2unique, unique)
print(unique_values)


```

```{r}
# Something is wrong with time
df$time <- as.POSIXct(df$time, origin = "1970-01-01", tz = "UTC")

```

```{r}
#Let's check for missing values in each column
missing_values <- sapply(df, function(x) sum(is.na(x)))
missing_values
```


```{r}

###19
# Step 1: Check how many missing values are in the 'time' column before fixing
missing_time_before <- sum(is.na(df$time))
cat("Missing values in 'time' before fixing:", missing_time_before, "\n")

# Step 2: Identify potential time-related columns
time_columns <- grep("date|time", names(df), ignore.case = TRUE, value = TRUE)
cat("Time-related columns:", time_columns, "\n")

# Step 3: If an alternative time column exists, use it to fill missing values
if (length(time_columns) > 0) {
  alternative_time_column <- time_columns[1]  # Use the first detected time-related column
  cat("Using", alternative_time_column, "to fill missing values.\n")
  
  # Convert alternative time column to datetime format with specific format if necessary
  # For example, if the format is "YYYY-MM-DD HH:MM:SS"
  df[[alternative_time_column]] <- as.POSIXct(df[[alternative_time_column]], format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
  
  # Fill missing values in 'time' with values from the alternative column
  df$time[is.na(df$time)] <- df[[alternative_time_column]][is.na(df$time)]
}

# Step 4: Check how many missing values are left after fixing
missing_time_after <- sum(is.na(df$time))
cat("Missing values in 'time' after fixing:", missing_time_after, "\n")

```
```{r}
#Delete columns datePostedString and description

df <- df[, !(names(df) %in% c("datePostedString", "description"))]

df$datePostedString <- NULL
df$description <- NULL

```

```{r}
#Print rows where any value is missing, displaying only the columns: event, time, and city.

###21
missing_rows <- df[is.na(df$event) | is.na(df$time) | is.na(df$city), c("event", "time", "city")]
print(missing_rows)
```


```{r}
#Change all values NaN in the event column to Unknown
df$event[is.na(df$event)] <- "Unknown"

```

```{r}
# Check for missing values in each column
missing_values <- sapply(df, function(x) sum(is.na(x)))
missing_values[missing_values > 0]

```

```{r}
#In all those columns replace zeros with NaNs
df[is.na(df)] <- NA  
df[df == 0] <- NA
```


```{r}
#Do all cities have unique cityId? Find cities wuth multiple IDs if any.
library(dplyr)
city_id_duplicates <- df %>%
  group_by(city) %>%
  summarise(unique_ids = n_distinct(cityId)) %>%
  filter(unique_ids > 1)
print(city_id_duplicates)

```
```{r}
#Does every city have at least one cityId? Identify any cities without a cityId if they exist.

cities_without_cityId <- df %>%
  filter(is.na(cityId)) %>%
  select(city) %>%
  distinct()
print(cities_without_cityId)

```
```{r}
#Replace NaN values in the cityId column with the corresponding cityId for cities that have a unique cityId. Report how many missing values where in the cityId column before and after the replacement.
library(dplyr)

# Step 1: Check how many missing values are in the 'cityId' column before fixing
missing_cityId_before <- sum(is.na(df$cityId))
cat("Missing values in 'cityId' before fixing:", missing_cityId_before, "\n")

# Step 2: Replace NaN values in 'cityId' with the corresponding 'cityId' for cities with unique 'cityId'
df <- df %>%
  group_by(city) %>%
  mutate(cityId = ifelse(is.na(cityId) & n_distinct(cityId) == 1, first(cityId[!is.na(cityId)]), cityId)) %>%
  ungroup()

# Step 3: Check how many missing values are in the 'cityId' column after the replacement
missing_cityId_after <- sum(is.na(df$cityId))
cat("Missing values in 'cityId' after fixing:", missing_cityId_after, "\n")


```
```{r}
#Identify all columns that contain exactly two distinct values and convert them to categorical

columns_with_two_distinct <- sapply(df, function(x) length(unique(x)) == 2)
cat("Columns with exactly two distinct values:\n")
print(names(df)[columns_with_two_distinct])
df[columns_with_two_distinct] <- lapply(df[columns_with_two_distinct], as.factor)
print(df)
```


```{r}
# Convert the 'countyId', 'cityId', and 'zipcode' columns into categorical (factor)
df$countyId <- as.factor(df$countyId)
df$cityId <- as.factor(df$cityId)
df$zipcode <- as.factor(df$zipcode)

print(df)


```
```{r}
# Check for duplicate rows and remove them, keeping only the first occurrence
df <- df[!duplicated(df), ]

cat("Number of rows after removing duplicates:", nrow(df), "\n")

```

```{r}
#Display a summary of the basic information for numerical, categorical and object features in this DataFrame.
# Select numerical columns and print the summary
numerical_summary <- summary(df[sapply(df, is.numeric)])

cat("Numerical Features Summary:\n")
print(numerical_summary)

```


#Explanatory Data Analysis
```{r}
library(ggplot2)
library(plotly)
library(dplyr)
library(tidyr)
library(lubridate)

```

```{r}
# Distribution of Price
library(ggplot2)
library(scales)

ggplot(df, aes(x = price)) +
  geom_histogram(bins = 50, fill = "#69b3a2", color = "black") +
  scale_x_continuous(labels = label_comma()) +
  coord_cartesian(xlim = c(0, 2000000)) +
  labs(
    title = "Distribution of Home Prices (Zoomed to $0â€“$2M)",
    x = "Price (USD)",
    y = "Count"
  ) +
  theme_minimal()



```
```{r}
library(scales)

# Distribution of Living Area (Limited & Formatted)
ggplot(df, aes(x = livingArea)) +
  geom_histogram(bins = 50, fill = "#ff9999", color = "black") +
  scale_x_continuous(
    limits = c(0, 10000),  # Adjust based on your data
    labels = comma
  ) +
  labs(
    title = "Distribution of Living Area",
    x = "Living Area (sqft)",
    y = "Count"
  ) +
  theme_minimal()


```

```{r}
library(ggplot2)
library(scales)

ggplot(df, aes(x = livingArea)) +
  geom_histogram(bins = 50, fill = "#ff9999", color = "black") +
  scale_x_continuous(labels = comma, limits = c(0, 10000)) +
  labs(
    title = "Distribution of Living Area",
    x = "Living Area (sqft)",
    y = "Count"
  ) +
  theme_minimal()




```
```{r}
#  Interactive Plot: Price vs. Living Area
 
library(scales)  # Make sure this is loaded

p <- ggplot(df, aes(x = livingArea, y = price, text = paste("City:", city))) +
  geom_point(alpha = 0.5, color = "#1f77b4") +
  scale_x_continuous(
    limits = c(0, 10000),
    labels = comma  # x-axis formatting
  ) +
  scale_y_continuous(
    labels = comma  # ðŸ‘‰ This is what was missing
  ) +
  labs(title = "Interactive Price vs. Living Area") +
  theme_minimal()

ggplotly(p)




```
```{r}
# Price Distribution by Home Type
library(ggplot2)
library(scales)  # ðŸ‘ˆ Needed for comma formatting

ggplot(df, aes(x = homeType, y = price)) +
  geom_boxplot(fill = "#f39c12") +
  scale_y_continuous(labels = comma) +  # ðŸ‘ˆ Adds commas
  labs(
    title = "Price Distribution by Home Type",
    x = "Home Type",
    y = "Price"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  theme_minimal()

```
```{r}
# Count of Listings by City (Top 10)
top_cities <- df %>%
  group_by(city) %>%
  summarise(listings = n()) %>%
  arrange(desc(listings)) %>%
  top_n(10, listings)

ggplot(top_cities, aes(x = reorder(city, listings), y = listings)) +
  geom_bar(stat = "identity", fill = "#3498db") +
  coord_flip() +
  labs(title = "Top 10 Cities by Listing Count", x = "City", y = "Number of Listings") +
  theme_minimal()


```




# EDA Using advanced data analytics techniques like ggplot2, plotly,e.t.c
```{r}
#Distribution of Numeric Features (Histograms / Density Plots)
library(ggplot2)

numeric_cols <- df %>%
  select(where(is.numeric)) %>%
  names()

# Plot histograms for the top 6 most important numeric features (customize as needed)
plot_list <- lapply(numeric_cols[1:6], function(colname) {
  ggplot(df, aes_string(x = colname)) +
    geom_histogram(fill = "#3498db", bins = 30, color = "white") +
    labs(title = paste("Distribution of", colname), x = colname, y = "Count") +
    theme_minimal()
})

# Print plots one by one
for (p in plot_list) {
  print(p)
}

```

```{r}
#Correlation Matrix (Heatmap)
library(ggcorrplot)

# Filter numeric variables
df_numeric <- df %>%
  select(where(is.numeric)) %>%
  na.omit()

# Compute correlation matrix
corr_matrix <- cor(df_numeric)

# Plot heatmap
ggcorrplot(corr_matrix, lab = TRUE, type = "lower", colors = c("#e74c3c", "white", "#2ecc71"))

```

```{r}
#Boxplot: Price vs City
library(ggplot2)
library(dplyr)

top_cities <- df %>%
  count(city, sort = TRUE) %>%
  top_n(10) %>%
  pull(city)

ggplot(df %>% filter(city %in% top_cities), aes(x = city, y = price)) +
  geom_boxplot(fill = "#9b59b6") +
  labs(title = "Price Distribution Across Top 10 Cities", x = "City", y = "Price") +
  theme_minimal() +
  scale_y_continuous(labels = scales::comma) +  # Prevent scientific notation
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

```{r}
# Interactive Scatter Plot with Plotly
# Get top 8 zipcodes by frequency
top_zipcodes <- df %>%
  count(zipcode, sort = TRUE) %>%
  top_n(8) %>%
  pull(zipcode)

# Filter the data for only those zipcodes
df_filtered <- df %>%
  filter(zipcode %in% top_zipcodes & !is.na(livingArea) & !is.na(price))

# Now plot
plot_ly(data = df_filtered, 
        x = ~livingArea, 
        y = ~price, 
        type = 'scatter', 
        mode = 'markers',
        color = ~zipcode,
        marker = list(size = 7, opacity = 0.6)) %>%
  layout(title = "Price vs Living Area (Top 8 ZIP Codes)",
         xaxis = list(title = "Living Area"),
         yaxis = list(title = "Price"))

```


# Research Question 1:

# (Can we predict the property price based on features such as living area, number of bedrooms, bathrooms, and city?)

```{r}
# Step 1: Filter and prepare data
# Keep only complete cases for the selected variables
model_data <- df %>%
  select(price, livingArea, bedrooms, bathrooms, city) %>%
  filter(!is.na(price), !is.na(livingArea), !is.na(bedrooms), !is.na(bathrooms), !is.na(city))

# Convert city to a factor (if not already)
model_data$city <- as.factor(model_data$city)

# Check structure
str(model_data)

```

```{r}
#Step 2: Build the linear regression model
# Build the model
lm_model <- lm(price ~ livingArea + bedrooms + bathrooms + city, data = model_data)

# Summary of the model
summary(lm_model)

```
```{r}
#Step 3: Evaluate model performance
# Predictions
model_data$predicted_price <- predict(lm_model, model_data)

# Compute RMSE (Root Mean Square Error)
rmse <- sqrt(mean((model_data$predicted_price - model_data$price)^2))
cat("RMSE of Linear Model:", rmse, "\n")


```

```{r}
# Step 4: Visualization - Actual vs Predicted Prices
library(scales)

ggplot(model_data, aes(x = price, y = predicted_price)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Actual vs Predicted Prices",
    x = "Actual Price",
    y = "Predicted Price"
  ) +
  scale_x_continuous(labels = label_comma()) +  # ðŸ‘ˆ No more exponential notation
  scale_y_continuous(labels = label_comma()) +
  theme_minimal()


```



# Research Question 2:

# (Can we identify different clusters of property types based on size, number of bedrooms/bathrooms, and price?)


```{r}
#Step 1: Select and scale features for clustering
library(dplyr)
library(ggplot2)

# Select numeric features for clustering
clustering_data <- df %>%
  select(livingArea, bedrooms, bathrooms, price) %>%
  filter(!is.na(livingArea), !is.na(bedrooms), !is.na(bathrooms), !is.na(price))

# Standardize the data
clustering_scaled <- scale(clustering_data)

# View scaled data
head(clustering_scaled)

```
```{r}
#Step 2: Determine the optimal number of clusters (Elbow Method)
wss <- vector()
for (k in 1:10) {
  kmeans_result <- kmeans(clustering_scaled, centers = k, nstart = 10)
  wss[k] <- kmeans_result$tot.withinss
}

# Plot the elbow curve
plot(1:10, wss, type = "b", pch = 19,
     xlab = "Number of Clusters (k)",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method for Optimal k")

```

```{r}
#  Step 3: Apply K-Means Clustering
set.seed(123)  # For reproducibility
kmeans_result <- kmeans(clustering_scaled, centers = 4, nstart = 25)

# Add cluster assignment to original data
clustering_data$cluster <- as.factor(kmeans_result$cluster)

```

```{r}
#Step 4: Visualize Clusters (Using First 2 Principal Components)
library(ggfortify)

autoplot(prcomp(clustering_scaled), 
         data = clustering_data, 
         colour = 'cluster',
         main = "K-Means Clustering Visualization (PCA Projection)") +
  theme_minimal()

```


```{r}
#cluster summary
cluster_summary <- clustering_data %>%
  group_by(cluster) %>%
  summarise(
    avg_price = mean(price),
    avg_livingArea = mean(livingArea),
    avg_bedrooms = mean(bedrooms),
    avg_bathrooms = mean(bathrooms),
    count = n()
  )

print(cluster_summary)

```




# Research Question 3:

# (What are the most important features influencing property prices?)

```{r}
#Step 1: Prepare the data
library(randomForest)

# Select relevant columns
rf_data <- df %>%
  select(price, livingArea, bedrooms, bathrooms, yearBuilt, zipcode, cityId) %>%
  filter(!is.na(price), !is.na(livingArea), !is.na(bedrooms), !is.na(bathrooms), !is.na(yearBuilt), !is.na(zipcode), !is.na(cityId))

# Convert categorical variables to factors
rf_data$zipcode <- as.factor(rf_data$zipcode)
rf_data$cityId <- as.factor(rf_data$cityId)

# Check structure
str(rf_data)

```

```{r}
#Step 2: Train the Random Forest Model

library(randomForest)
rf_data_clean <- rf_data %>%
  select(-zipcode, -cityId)
set.seed(42)

rf_model <- randomForest(price ~ ., data = rf_data_clean, importance = TRUE, ntree = 200)
print(rf_model)

```
```{r}
#Step 3: Plot Variable Importance
# Variable importance plot
varImpPlot(rf_model, type = 1, main = "Feature Importance (Random Forest)")

```
```{r}
#Step 4: Model Performance
# Predict and evaluate model
rf_data$predicted_price <- predict(rf_model, rf_data)

# Calculate RMSE
rf_rmse <- sqrt(mean((rf_data$predicted_price - rf_data$price)^2))
cat("RMSE for Random Forest model:", rf_rmse, "\n")

```

```{r}
#Compare Actual vs Predicted
library(ggplot2)
library(scales)  # for label_number()

ggplot(rf_data, aes(x = price, y = predicted_price)) +
  geom_point(alpha = 0.4, color = "darkgreen") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_continuous(labels = label_number()) +
  scale_y_continuous(labels = label_number()) +
  labs(title = "Actual vs Predicted Prices (Random Forest)",
       x = "Actual Price",
       y = "Predicted Price") +
  theme_minimal()


```

#Research Question 4: 

#(What features are most important in predicting house prices?)

```{r}
# Extract variable importance
importance_mat <- importance(rf_model)
importance_df <- as.data.frame(importance_mat)

# Add feature names
importance_df$Feature <- rownames(importance_df)


print(colnames(importance_df))

# Sort by first column (assuming regression, using IncNodePurity)
col_to_use <- colnames(importance_df)[1]

# Get top 15 features
top_features <- importance_df %>%
  arrange(desc(!!sym(col_to_use))) %>%
  slice(1:15)

# Plot
library(ggplot2)

ggplot(top_features, aes(x = reorder(Feature, !!sym(col_to_use)), y = !!sym(col_to_use))) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 15 Most Important Features in Predicting House Prices",
    x = "Features",
    y = col_to_use
  ) +
  theme_minimal()



```


#Model Evaluation (Performance Metrics + Residual Analysis)

```{r}

#1. Evaluate Model Performance (RMSE, MAE, R-squared)

# Load required libraries
library(randomForest)
library(forcats)
library(Metrics)

# Identify factor columns with too many categories
high_card_cols <- sapply(rf_data, function(x) is.factor(x) && length(unique(x)) > 53)
cat("High-cardinality columns:\n")
print(names(high_card_cols[high_card_cols]))

# Lump high-cardinality factors to top 20 levels
for (col in names(high_card_cols[high_card_cols])) {
  rf_data[[col]] <- fct_lump(factor(rf_data[[col]]), n = 20)
}

# Set seed and split data
set.seed(42)
sample_indices <- sample(1:nrow(rf_data), size = 0.8 * nrow(rf_data))
train_data <- rf_data[sample_indices, ]
test_data <- rf_data[-sample_indices, ]

# Separate test labels
y_test <- test_data$price

# Train Random Forest model
rf_model <- randomForest(price ~ ., data = train_data, importance = TRUE, ntree = 200)

# Predict on test set
rf_predictions <- predict(rf_model, newdata = test_data)

# Evaluate model
rmse_val <- rmse(y_test, rf_predictions)
mae_val <- mae(y_test, rf_predictions)
r2_val <- cor(y_test, rf_predictions)^2

# Print metrics
cat("Random Forest Performance on Test Set:\n")
cat("RMSE:", round(rmse_val, 2), "\n")
cat("MAE:", round(mae_val, 2), "\n")
cat("R-squared:", round(r2_val, 4), "\n")


```
```{r}

# Residual Analysis plot(Actual - predicted)

ggplot(residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "darkorange") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "steelblue") +
  labs(
    title = "Residuals vs Predicted Values",
    x = "Predicted Price",
    y = "Residuals (Actual - Predicted)"
  ) +
  scale_x_continuous(labels = label_comma()) +
  scale_y_continuous(labels = label_comma()) +
  theme_minimal()


```


```{r}
## Conclusion & Insights

#- We successfully cleaned and preprocessed the real estate dataset.
#- Exploratory data analysis revealed temporal trends, geographic clustering, and important relationships.
#- We addressed three research questions using:
 # - Linear Regression
 # - K-Means Clustering
 # - Random Forest Regression
#- Model evaluation showed that the Random Forest outperformed other models in prediction accuracy.
#- Key predictive features included `livingArea`, `bedrooms`, `bathrooms`, and `zipcode`.
#- Residual analysis confirmed the model's reliability, with minimal bias.

```

